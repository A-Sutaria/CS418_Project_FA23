{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Project-Name\n",
    "\n",
    "Authors:\n",
    "\n",
    "* Arpan Sutaria\n",
    "* Devesh Patel\n",
    "* Htin Linn Htoo Than\n",
    "* Jay Patel\n",
    "* Kevinkumar Patel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Introduction\n",
    "\n",
    "TODO: an introduction that discusses the data and related problems that you are investigating."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Any changes since the proposal\n",
    "Describe the previous project in brief. Mention why we shifted to our current project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data - Htin Linn\n",
    "We mainly obtained the data by downloading from https://www.zillow.com/research/data/ according to the geogrpahy and data types needed. We found an abundance of data on house prices on which we are able to make hypotheses on. However, since many of the datasets have missing values, we each had to clean the that we chose to work on so that there aren't any errors when doing visualizations or machine learning analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (EDA) - Htin Linn\n",
    "\n",
    "The datasets we used for this project mainly consists of numerical values which are prices of different homes over a period of time and variables which are different regions and states. Since our main focus is to compare prices across different variables over a different period of time, we had to make sure our datasets included exactly the type of data we needed and we needed to clean to filter out any missing values.\n",
    "\n",
    "\n",
    "The 6 main different datasets are The Zillow Home Value Index (ZHVI), the Zillow Home Value Forecast (ZHVF), the Zillow Observed Rent Index (ZORI), Inventory, List and Sale Prices and Sales Count and Price Cuts. Each dataset is also divided into multiple data types across different geography values such as Metro & U.S, State, County, City, Zip Code and Neighbourhood. However, the issue we faced was that not all datasets have the each of the geography value except for Metro & U.S so we decided to mainly use the datasets which have the Metro & U.S values.\n",
    "\n",
    "For ZHVI, the dataset is the measure of the typical home value based on their size and tier and market changes across a given region and housing type. We focused on all homes in the United States as a whole throughout the years to give a breif idea about the dataset and its trend.\n",
    "\n",
    "<img src=\"EDA_Graphs\\EDA for ZHVI.png\">\n",
    "\n",
    "\n",
    "\n",
    "For ZHVF, the dataset is a forecast of the Zillow Home Value Index (ZHVI). ZHVF is created using the all homes, mid-tier cut of ZHVI and is available both raw and smoothed, seasonally adjusted divided up into time ranges of a month-ahead, quarter-ahead and year-ahead which the BaseDate is September 31,2023. Therefore, the dataset contains the forest of homes' prices in October 31,2023, December 31,2023 and August 30,2024.  This simple graph below shows the overall trend of the forecast all homes in the United States. \n",
    "\n",
    "<!-- ![EDA for ZHVF.png](<attachment:EDA for ZHVF.png>) -->\n",
    "<img src=\"EDA_Graphs\\EDA for ZHVF.png\">\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "For ZORI, the datasets are the measure of the typical rent price across a given region. Below is the graph of the rent prices of all homes in the United States as a whole throughout the years to give an idea about the dataset and its trend.\n",
    "\n",
    "<img src=\"EDA_Graphs\\EDA for ZORI.png\">\n",
    "<!-- ![EDA for ZORI.png](<attachment:EDA for ZORI.png>) -->\n",
    "\n",
    "\n",
    "\n",
    "For Inventory, the datasets' topics include For-Sale Inventory which is the count of unique listings that were active at any time in a given month, New listings which shows how many new listings have come on the market in a given month, newly pending listings which are the count of listings that changed from for-sale to pending status on Zillow.com in a given time period and Days to Pending which contains how long it takes homes in a region to change to pending status on Zillow.com after first being shown as for sale for all homes weekly or monthly. For example, the graph below shows the overall trend of the For-Sale Inventory for all homes montly in the United States.\n",
    "\n",
    "<!-- ![EDA for Inventory.png](<attachment:EDA for Inventory.png>) -->\n",
    "<img src=\"EDA_Graphs\\EDA for Inventory.png\">\n",
    "\n",
    "\n",
    "For List and Sale Prices, the topics are median list price of homes across various geographies, median sale price at which homes across various geographies were sold, ratio of sale vs. final list price, ratio of sales where sale price below/above the final list price not including the homes sold for exactly the list price and the number of days between the listing going pending and the sale date. The graph below shows the overall trend of the median price at which homes across the United States were listed.\n",
    "\n",
    "<!-- ![EDA for Median Price of Homes in U.S.png](<attachment:EDA for Median Price of Homes in U.S.png>) -->\n",
    "<img src=\"EDA_Graphs\\EDA for Median Price of Homes in U.S.png\">\n",
    "\n",
    "\n",
    "For Sales Count and Price Cuts, the topics are The Sales Count Nowcast which is the estimated number of unique properties that sold during the month after accounting for the latency between when sales occur and when they are reported, Share of Listings With a Price Cut which is the number of unique properties with a list price at the end of the month that’s less than the list price at the beginning of the month, divided by the number of unique properties with an active listing at some point during the month and Price Cuts which are the mean and median price cut for listings in a given region during a given time period. The graph below shows the overall trend of number of unique properties that are sold during the month after accounting for the latency between when sales occur and when they are reported in the United States.\n",
    "\n",
    "<!-- ![EDA for Sales Count.png](<attachment:EDA for Sales Count.png>) -->\n",
    "<img src=\"EDA_Graphs\\EDA for Sales Count.png\">\n",
    "\n",
    "\n",
    "Overall, the data we obtained have an abundance of information already which makes it easier for us to make hypotheses on and compare different datasets across different variables. The overall trend of all datasets seems to be that the prices always gets higher throughout time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization (Needed at least 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization 1 (Arpan)\n",
    "\n",
    "### Hypothesis\n",
    "There is a co-relation between schools and home values in a given area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# in case you don't have this library installed\n",
    "# %pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ZHVI data\n",
    "zhvi_data = pd.read_csv(\"datasets\\Zip_zhvi_uc_sfrcondo_tier_0.33_0.67_sm_sa_month.csv\")\n",
    "zhvi_data = zhvi_data.drop('SizeRank', axis=1)\n",
    "zhvi_data = zhvi_data.drop('StateName', axis=1)\n",
    "zhvi_data = zhvi_data.rename(columns={'RegionName': 'ZIP'})  # Rename the 'RegionName' column to 'ZIP' in zhvi_data\n",
    "\n",
    "# Load school data\n",
    "school_data = pd.read_excel(\"datasets\\EDGE_GEOCODE_PUBLICSCH_2122.xlsx\")\n",
    "\n",
    "# Creating two groups so that we can compare them on the graph and prove/disprove our hypothesis\n",
    "zhvi_with_schools = zhvi_data.merge(\n",
    "    school_data[['ZIP', 'STATE', 'NAME']],\n",
    "    on='ZIP',\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "# Create 'zhvi_without_schools' dataframe\n",
    "zhvi_without_schools = zhvi_data[~zhvi_data['ZIP'].isin(school_data['ZIP'])]\n",
    "\n",
    "# Select the columns of interest for both dataframes\n",
    "columns_of_interest = [\n",
    "    'RegionID', 'ZIP', 'State', 'City', 'Metro', 'CountyName', '2023-09-30'\n",
    "]\n",
    "\n",
    "zhvi_with_schools = zhvi_with_schools[columns_of_interest + ['NAME']]  # adding name of the schools for more clarity\n",
    "zhvi_without_schools = zhvi_without_schools[columns_of_interest]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(zhvi_with_schools['2023-09-30'], bins=20, alpha=0.5, color='blue', label='With Schools')\n",
    "plt.hist(zhvi_without_schools['2023-09-30'], bins=20, alpha=0.5, color='red', label='Without Schools')\n",
    "plt.xlabel('Median Home Value (ZHVI) at 10^6')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of ZHVI with and without Schools')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "If we look at the Bar chart plot, it is very obvious that there is a huge difference between the two groups: Houses with schools in the same neighborhood (ZIP Code), and Houses without schools in the same neighborhood (ZIP Code). It is observed that houses in the same ZIP Code region as a school had dramatically high Zillow Home Value Index (ZHVI) as of 09/30/2023 than those of houses in a ZIP Code region with no schools. This proves my hypothesis that there is a co-relation between houses and schools. The conclusion is that houses with schools nearby have a higher ZHVI. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization 2 (Devesh)\n",
    "### Hypothesis\n",
    "There is a correlation between ZHVI and ZORI, suggesting that home values influence rental prices?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "zhvi_data = pd.read_csv(\"Metro_zhvi_uc_sfrcondo_tier_0.33_0.67_sm_sa_month.csv\")\n",
    "zori_data = pd.read_csv(\"Metro_zori_sm_month.csv\")\n",
    "\n",
    "# For simplicity, we'll focus on the most recent month's data for both indices, which is September 2023.\n",
    "\n",
    "# Extract the most recent ZHVI and ZORI values for each region\n",
    "zhvi_latest = zhvi_data[['RegionName', '2023-09-30']].rename(columns={'2023-09-30': 'ZHVI'})\n",
    "zori_latest = zori_data[['RegionName', '2023-09-30']].rename(columns={'2023-09-30': 'ZORI'})\n",
    "\n",
    "# Merge the datasets on the RegionName field\n",
    "combined_zhvi_zori = pd.merge(zhvi_latest, zori_latest, on='RegionName')\n",
    "\n",
    "# Calculate the correlation between ZHVI and ZORI for the merged data\n",
    "correlation = combined_zhvi_zori[['ZHVI', 'ZORI']].corr()\n",
    "\n",
    "# We will plot only the top 10 most populous regions for clarity in the visualization\n",
    "top_regions = combined_zhvi_zori.nlargest(10, 'ZHVI')\n",
    "\n",
    "# Create scatter plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.regplot(x='ZHVI', y='ZORI', data=top_regions)\n",
    "\n",
    "plt.title('Scatter Plot of ZHVI vs ZORI for Top 10 Regions')\n",
    "plt.xlabel('Zillow Home Value Index (ZHVI)')\n",
    "plt.ylabel('Zillow Observed Rent Index (ZORI)')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "The scatter plot with a trend line for the top 10 regions by home value index (ZHVI) has been created, showing a strong positive correlation between the Zillow Home Value Index (ZHVI) and the Zillow Observed Rent Index (ZORI) with a correlation coefficient of approximately 0.88. This suggests a significant relationship where, generally, as home values increase, rental prices also tend to increase in these regions.​​"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization 3 (Htin Linn)\n",
    "### Hypothesis\n",
    "Listings that undergo a price cut stay on the market longer and have a lower Sale-to-List Ratio, suggesting that initial pricing is crucial for a quick sale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Reading Data\n",
    "daystoclose = pd.read_csv('Metro_mean_days_to_close_uc_sfrcondo_month.csv')\n",
    "saletolist = pd.read_csv('Metro_mean_sale_to_list_uc_sfrcondo_month.csv')\n",
    "\n",
    "# Most Recent Data\n",
    "dtc_latest = daystoclose[['RegionName', '2023-08-31']].rename(columns={'2023-08-31': 'dtc'})\n",
    "stl_latest = saletolist[['RegionName', '2023-08-31']].rename(columns={'2023-08-31': 'stl'})\n",
    "\n",
    "# Box plots\n",
    "plt.figure(figsize=(10, 6))  \n",
    "sns.boxplot(data=dtc_latest, palette=\"Set3\")\n",
    "plt.title(\"Box Plot of Days to Close for Homes with and without Price Cuts\")\n",
    "plt.xlabel(\"Price Cut\")\n",
    "plt.ylabel(\"Mean Days to Close\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))  \n",
    "sns.boxplot(data=stl_latest, palette=\"Set1\")\n",
    "plt.title(\"Box Plot of Sale-to-List Ratio for Homes with and without Price Cuts\")\n",
    "plt.xlabel(\"Price Cut\")\n",
    "plt.ylabel(\"Sale-to-List Ratio\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "The first box plot compares 'Days to Close' for homes with and without price cuts, while the second one compares the 'Sale-to-List Ratio' for the same groups. As we can see from the box plots, most of the listings that undergo a price cut stay on the market longer and have a lower Sale-to-List Ratio, suggesting that initial pricing is crucial for a quick sale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization 4 (Jay)\n",
    "### Hypothesis\n",
    "Do ZHVI and for-sale inventory levels exhibit seasonal patterns, and if so, how do they vary by region and housing tier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import seaborn as sns\n",
    "\n",
    "# Loading the CSV file into a Pandas DataFrame\n",
    "inventory = pd.read_csv(\"Inventory.csv\")\n",
    "days_pending = pd.read_csv(\"days_pending.csv\")\n",
    "\n",
    "# Melt the datasets to convert them from wide to long format\n",
    "inventory_melted = inventory.melt(id_vars=['RegionID', 'RegionName'], var_name='Date', value_name='Inventory')\n",
    "days_on_market_melted = days_pending.melt(id_vars=['RegionID', 'RegionName'], var_name='Date', value_name='DaysOnMarket')\n",
    "first_five_regions = inventory['RegionName'][2:6].tolist()\n",
    "\n",
    "inventory_first_five = inventory_melted[inventory_melted['RegionName'].isin(first_five_regions)]\n",
    "days_on_market_first_five = days_on_market_melted[days_on_market_melted['RegionName'].isin(first_five_regions)]\n",
    "merged_data_first_five = pd.merge(inventory_first_five, days_on_market_first_five, on=['RegionName', 'Date'])\n",
    "\n",
    "# Convert 'Date' to datetime and sort the data\n",
    "merged_data_first_five['Date'] = pd.to_datetime(merged_data_first_five['Date'], errors='coerce')\n",
    "\n",
    "merged_data_first_five = merged_data_first_five.dropna(subset=['Date']).sort_values(['RegionName', 'Date'])\n",
    "merged_data_first_five.dropna(subset=['Inventory', 'DaysOnMarket'], inplace=True)\n",
    "\n",
    "# Set style\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "X = merged_data_first_five['Inventory'].values.reshape(-1, 1)\n",
    "Y = merged_data_first_five['DaysOnMarket'].values\n",
    "\n",
    "# linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X, Y)\n",
    "\n",
    "# values for the trend line\n",
    "trend_line = model.predict(X)\n",
    "\n",
    "# scatter plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X, Y, color='c', label='Data points')\n",
    "plt.plot(X, trend_line, color='blue', label='Trend Line')\n",
    "\n",
    "plt.title('Correlation between Inventory Levels and Days on Market for the US (Top 5 regions)')\n",
    "plt.xlabel('Inventory Levels')\n",
    "plt.ylabel('Days on Market')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization 5 (Kevin)\n",
    "### Hypothesis\n",
    "The 4 bedroom home values are growing at a faster rate than the home values with lesser bedrooms, indicating an increasing disparity in the housing market."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load your data into pandas DataFrames from CSV files\n",
    "one_bed = pd.read_csv('Metro_1_bed.csv')\n",
    "two_bed = pd.read_csv('Metro_2_bed.csv')\n",
    "three_bed = pd.read_csv('Metro_3_bed.csv')\n",
    "four_bed = pd.read_csv('Metro_4_bed.csv')\n",
    "\n",
    "chicago_one = one_bed[one_bed['RegionName'] == 'Chicago, IL']\n",
    "chicago_one_from_2000_to_end = chicago_one.iloc[0][one_bed.columns.get_loc('2000-01-31'):]\n",
    "chicago_two = two_bed[two_bed['RegionName'] == 'Chicago, IL']\n",
    "chicago_two_from_2000_to_end = chicago_two.iloc[0][two_bed.columns.get_loc('2000-01-31'):]\n",
    "chicago_three = three_bed[three_bed['RegionName'] == 'Chicago, IL']\n",
    "chicago_three_from_2000_to_end = chicago_three.iloc[0][three_bed.columns.get_loc('2000-01-31'):]\n",
    "chicago_four = four_bed[four_bed['RegionName'] == 'Chicago, IL']\n",
    "chicago_four_from_2000_to_end = chicago_four.iloc[0][four_bed.columns.get_loc('2000-01-31'):]\n",
    "\n",
    "# Increase default font size and line width for better readability\n",
    "sns.set_context('talk', font_scale=0.8)\n",
    "\n",
    "# Set the Seaborn theme\n",
    "sns.set_style('whitegrid')\n",
    "sns.set_palette('pastel')\n",
    "\n",
    "# Create the figure and the line plot\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "# Plot the data for each bedroom type\n",
    "for df, label in [(chicago_one_from_2000_to_end, '1 Bedroom'),\n",
    "                 (chicago_two_from_2000_to_end, '2 Bedroom'),\n",
    "                 (chicago_three_from_2000_to_end, '3 Bedroom'),\n",
    "                 (chicago_four_from_2000_to_end, '4 Bedroom')]:\n",
    "\n",
    "    # Convert index to datetime\n",
    "    dates = pd.to_datetime(df.index)\n",
    "\n",
    "    # Plot with Seaborn\n",
    "    sns.lineplot(x=dates, y=df.values, label=label, linewidth=2.5)\n",
    "\n",
    "# Set the x-axis to display years only\n",
    "plt.gca().xaxis.set_major_locator(mdates.YearLocator())\n",
    "plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "\n",
    "# Set y-axis limits and labels\n",
    "plt.ylim(50000, plt.gca().get_ylim()[1]) # Set minimum y-value to 50000 and maximum to auto\n",
    "y_max = int(plt.gca().get_ylim()[1] // 50000 * 50000) # Find the nearest number less than max that is divisible by 50000\n",
    "if y_max < plt.gca().get_ylim()[1]: # If the max y-value is not a multiple of 50000, add another step\n",
    "    y_max += 50000\n",
    "plt.yticks(np.arange(50000, y_max, 50000))\n",
    "\n",
    "# Format the y-axis labels to include commas for thousands\n",
    "plt.gca().get_yaxis().set_major_formatter(plt.matplotlib.ticker.FuncFormatter(lambda x, p: format(int(x), ',')))\n",
    "\n",
    "# Rotate x-axis labels if needed\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Set plot title and labels\n",
    "plt.title('Chicago Home Value Trends by Bedroom Count', fontsize=18)\n",
    "plt.xlabel('Year', fontsize=16)\n",
    "plt.ylabel('ZHVI (USD)', fontsize=16)\n",
    "\n",
    "# Show legend with a title and increase its font size\n",
    "plt.legend(title='Bedroom Count', title_fontsize='14', fontsize='12', loc='upper left')\n",
    "\n",
    "# Set the background color\n",
    "plt.gca().set_facecolor('#f5f5f5')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Analysis (needed at least 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Analysis 1 (Devesh)\n",
    "### ML Use Case\n",
    "Develop a predictive model that forecasts home values (ZHVI) based on various features such as location, home size, number of bedrooms, and historical trends. This is a regression problem where the target variable is continuous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Load the dataset\n",
    "zhvi_data = pd.read_csv(\"Metro_zhvi_uc_sfrcondo_tier_0.33_0.67_sm_sa_month.csv\")\n",
    "\n",
    "# Impute missing values using the median\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "model_data_numeric = zhvi_data.select_dtypes(include=[np.number])  # Select only numeric columns\n",
    "model_data_imputed = imputer.fit_transform(model_data_numeric)\n",
    "model_data_imputed_df = pd.DataFrame(model_data_imputed, columns=model_data_numeric.columns)\n",
    "\n",
    "# Prepare the historical ZHVI data for modeling\n",
    "# We'll use the past 12 months of ZHVI values as features to predict the next month's ZHVI value\n",
    "X = model_data_imputed_df.iloc[:, :-13].values\n",
    "y = model_data_imputed_df.iloc[:, -13].values  # We use -13 to predict the ZHVI 12 months ahead\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Initialize the Gradient Boosting Regressor model\n",
    "gbr = GradientBoostingRegressor()\n",
    "\n",
    "# Train the model on the scaled dataset\n",
    "gbr.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict the ZHVI values on the test set using the trained model\n",
    "y_pred_gbr = gbr.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model using the test set\n",
    "mae_gbr = mean_absolute_error(y_test, y_pred_gbr)\n",
    "rmse_gbr = np.sqrt(mean_squared_error(y_test, y_pred_gbr))\n",
    "r2_gbr = r2_score(y_test, y_pred_gbr)\n",
    "\n",
    "print(mae_gbr, rmse_gbr, r2_gbr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Analysis 2 (Arpan)\n",
    "### ML Use Case\n",
    "Classify regional housing markets as \"hot,\" \"stable,\" or \"cold\" based on current market indicators such as Days to Pending, Inventory Levels, Sale-to-List Ratio, and historical ZHVI trends. This is a classification problem where the target variable is categorical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Data Acquisition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "\n",
    "# Would be looking at four features specifically\n",
    "# [Days_to_Pending, Inventory_Levels, Sale_to_List_Ratio, ZHVI_Trends, Market_Temperature (Target/Label)]\n",
    "days_to_pending = pd.read_csv(\"ml_datasets/days_to_pending_median_monthly_metro.csv\")\n",
    "inventory_levels = pd.read_csv(\"ml_datasets/inventory_monthly_metro.csv\")\n",
    "sale_to_list_ratio = pd.read_csv(\"ml_datasets/sale_to_list_median_montly_metro.csv\")\n",
    "zhvi_trends = pd.read_csv(\"ml_datasets/zhvi_monthly_metro.csv\")\n",
    "sales_count = pd.read_csv(\"ml_datasets/sales_count_monthly_metro.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Would be cleaning out data and making it balanced.\n",
    "# Firstly considering the same time for data.\n",
    "# ZHVI has more data than other data sets. So we would be not considering a lot of columns from that dataframe.\n",
    "# We want to consider data from '2018-03-31' to '2023-09-30' (Undersampling)\n",
    "# inventory_levels end at '2023'08'31', so need to impute values for that column\n",
    "date_range = pd.date_range(start='2018-03-31', end='2023-09-30', freq='M')\n",
    "\n",
    "# Cleaning days_to_pending\n",
    "days_to_pending = days_to_pending.drop('SizeRank', axis=1)\n",
    "days_to_pending = days_to_pending[days_to_pending['RegionName'] != 'United States']\n",
    "# print(\"days_to_pending\")\n",
    "# print(days_to_pending.columns)  # all columns (allegedly across all dataframes)\n",
    "\n",
    "# Cleaning inventory_levels\n",
    "inventory_levels = inventory_levels.drop('SizeRank', axis=1)\n",
    "inventory_levels = inventory_levels[inventory_levels['RegionName'] != 'United States']\n",
    "# Imputing data in inventory_levels\n",
    "# Calculate the median of the existing values in the DataFrame for the '2023-09-30' column\n",
    "column_index_i_l = inventory_levels.columns.get_loc('2018-03-31') # Find the index of the '2018-03-31' column\n",
    "median_value = inventory_levels.iloc[:, column_index_i_l:-1].median(axis=1)  # column_index_i_l represents the index of the '2018-03-31' column\n",
    "inventory_levels['2023-09-30'] = median_value  # Add the '2023-09-30' column and impute placeholder values (e.g., 0) for now\n",
    "# print(days_to_pending.columns == inventory_levels.columns) # [... True ...]\n",
    "\n",
    "# Cleaning sale_to_list_ratio\n",
    "sale_to_list_ratio = sale_to_list_ratio.drop('SizeRank', axis=1)\n",
    "sale_to_list_ratio = sale_to_list_ratio[sale_to_list_ratio['RegionName'] != 'United States']\n",
    "# Imputing data in sale_to_list_ratio\n",
    "# Calculate the median of the existing values in the DataFrame for the '2023-09-30' column\n",
    "column_index_s_t_l = sale_to_list_ratio.columns.get_loc('2018-03-31') # Find the index of the '2018-03-31' column\n",
    "median_value = sale_to_list_ratio.iloc[:, column_index_s_t_l:-1].median(axis=1)  # column_index_s_t_l represents the index of the '2018-03-31' column\n",
    "sale_to_list_ratio['2023-09-30'] = median_value  # Add the '2023-09-30' column and impute placeholder values (e.g., 0) for now\n",
    "# print(\"\\nsale_to_list_ratio\")\n",
    "# print(inventory_levels.columns == sale_to_list_ratio.columns)  # [... True ...]\n",
    "\n",
    "# Cleaning zhvi_trends\n",
    "zhvi_trends = zhvi_trends.drop('SizeRank', axis=1)\n",
    "zhvi_trends = zhvi_trends[zhvi_trends['RegionName'] != 'United States']\n",
    "zhvi_trends = zhvi_trends[['RegionID', 'RegionName', 'RegionType', 'StateName'] + date_range.strftime('%Y-%m-%d').tolist()]\n",
    "# print(\"\\nzhvi_trends\")\n",
    "# print(sale_to_list_ratio.columns == zhvi_trends.columns)  # [... True ...]\n",
    "\n",
    "# Cleaning sales_count\n",
    "sales_count = sales_count.drop('SizeRank', axis=1)\n",
    "sales_count = sales_count[sales_count['RegionName'] != 'United States']\n",
    "sales_count = sales_count[['RegionID', 'RegionName', 'RegionType', 'StateName'] + date_range.strftime('%Y-%m-%d').tolist()]\n",
    "# print(zhvi_trends.columns == sales_count.columns)  # [... True ...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Data Processing\n",
    "Creating another dataframe market_temperature which would be our Target label market temperature determines whether the market is hot/stable/cold. In real world it is based on a lot of features, but for this project we are only considering two features to determine market temperature: sales count and inventory levels.\n",
    "\n",
    "$ market\\_temperature = sales\\_count / inventory\\_levels $\n",
    "\n",
    "* If low inventory yet more sales, implies there's more demand in the market, hence the market is 'hot'.\n",
    "* If moderate inventory and moderate sales, implies the market is 'stable'.\n",
    "* If high inventory yet less sales, implies there's less demand in the market, hence the market is'cold'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have sales_count and inventory_levels, we can create market_temperature\n",
    "# Define the common columns for merging\n",
    "common_columns = ['RegionID', 'RegionName', 'RegionType', 'StateName']\n",
    "\n",
    "# Merge the dataframes on the common columns\n",
    "market_temperature = pd.merge(sales_count, inventory_levels, on=common_columns, suffixes=('_sales', '_inventory'))\n",
    "\n",
    "# Iterate over the date columns to calculate market temperature\n",
    "for column in sales_count.columns.difference(common_columns):\n",
    "    market_temperature[column] = market_temperature[f\"{column}_sales\"] / market_temperature[f\"{column}_inventory\"]\n",
    "\n",
    "# Drop the redundant columns\n",
    "market_temperature = market_temperature.drop(market_temperature.filter(like='_sales').columns, axis=1)\n",
    "market_temperature = market_temperature.drop(market_temperature.filter(like='_inventory').columns, axis=1)\n",
    "\n",
    "# Now we have the market_temperature in values\n",
    "# market_temperature.head(5)\n",
    "\n",
    "# Define the mapping function\n",
    "def map_to_category(value):\n",
    "    if pd.notna(value):\n",
    "        if value <= 0.333333:\n",
    "            return 'cold'\n",
    "        elif value <= 0.666666:\n",
    "            return 'stable'\n",
    "        else:\n",
    "            return 'hot'\n",
    "    return value\n",
    "\n",
    "# Apply the mapping function to convert decimal values to categories\n",
    "market_temperature_categorical = market_temperature.copy()\n",
    "market_temperature_categorical.iloc[:, 4:] = market_temperature.iloc[:, 4:].apply(lambda x: x.map(map_to_category))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Machine Learning (Decision Tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Features: [Days_to_Pending, Inventory_Levels, Sale_to_List_Ratio, ZHVI_Trends]\n",
    "X = market_temperature[['2018-03-31', '2018-04-30', '2018-05-31', '2018-06-30', '2018-07-31', '2018-08-31',\n",
    "                        '2018-09-30', '2018-10-31', '2018-11-30', '2018-12-31', '2019-01-31', '2019-02-28',\n",
    "                        '2019-03-31', '2019-04-30', '2019-05-31', '2019-06-30', '2019-07-31', '2019-08-31',\n",
    "                        '2019-09-30', '2019-10-31', '2019-11-30', '2019-12-31', '2020-01-31', '2020-02-29',\n",
    "                        '2020-03-31', '2020-04-30', '2020-05-31', '2020-06-30', '2020-07-31', '2020-08-31',\n",
    "                        '2020-09-30', '2020-10-31', '2020-11-30', '2020-12-31', '2021-01-31', '2021-02-28',\n",
    "                        '2021-03-31', '2021-04-30', '2021-05-31', '2021-06-30', '2021-07-31', '2021-08-31',\n",
    "                        '2021-09-30', '2021-10-31', '2021-11-30', '2021-12-31', '2022-01-31', '2022-02-28',\n",
    "                        '2022-03-31', '2022-04-30', '2022-05-31', '2022-06-30', '2022-07-31', '2022-08-31',\n",
    "                        '2022-09-30', '2022-10-31', '2022-11-30', '2022-12-31', '2023-01-31', '2023-02-28',\n",
    "                        '2023-03-31', '2023-04-30', '2023-05-31', '2023-06-30', '2023-07-31', '2023-08-31',\n",
    "                        '2023-09-30']]\n",
    "\n",
    "# Target variable: Market_Temperature (categorical)\n",
    "y = market_temperature_categorical['2023-09-30']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# Initialize the Decision Tree model\n",
    "dt_model = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Train the model\n",
    "dt_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = dt_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "classification_rep = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(\"Classification Report:\\n\", classification_rep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reflection\n",
    "TODO:\n",
    "○ What is the most challenging part of the project that you’ve encountered so far?\n",
    "○ What are your initial insights?\n",
    "○ Are there any concrete results you can show at this point? If not, why not?\n",
    "○ Going forward, what are the current biggest problems you’re facing?\n",
    "○ Do you think you are on track with your project? If not, what parts do you need to dedicate more time to?\n",
    "○ Given your initial exploration of the data, is it worth proceeding with your project, why? If not, how will you move forward (method, data etc)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Step\n",
    "TODO: Concrete plans and goals for the next month"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
