{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Let's get real-estate!\n",
    "\n",
    "## Authors | NetID\n",
    "* Arpan Sutaria | asutar5@uic.edu\n",
    "* Devesh Patel | dpate320@uic.edu\n",
    "* Htin Linn Htoo Than | hthan2@uic.edu\n",
    "* Jay Patel | jpate281@uic.edu\n",
    "* Kevinkumar Patel | kpate413@uic.edu\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Introduction\n",
    "\n",
    "This project is more focused towards analyzing the real estate market and figuring out the trends and patterns throughout history. We start by looking the data overall and analyzing it through EDA. Then we came up with our hypothesis that were based off on trends that we see in real world. We used the obtained data to prove or disprove our hypothesis through visualizations. Lastly, we have two machine learning models. One is a predicting model which predicts the value indices of properties. The other is a classifying model that classifies a region's market as hot, stable or cold. This project is essentially brought up with the idea that what kind of things would a property manager look at or consider before getting into the market. Or, what kind of insights would help an existing property manager to make more sound decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Any changes since the proposal\n",
    "Yes, a large one. Initially we were planning to do a data science project that analyzed the traffic patterns on Chicago's freeways. The end goal would be to predict at what time should one leave to get to their destination on time with the least amount of time spent in traffic. Since our goal was too specific, it was very hard to find data that fit our idea. Eventually we had to gave up this idea, not because it was a bad one, but because there was no data that we could base our project on.\n",
    "\n",
    "Then we came up with the idea of diving into the real-estate world as that was a common theme among most team members. We all agreed on this idea, and we asked ourselves questions like what do we wanna know about the real-estate market, what insights would a property manager find valuable, etc. This was the foundation of the current idea."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data - Htin Linn\n",
    "\n",
    "Our main source of data is Zillow's public dataset available here: https://www.zillow.com/research/data/. There's a lot of data available, more than we need for the scope of this project. Each dataset is an attribute of feature of the property. Some of the major ones we are using are: Zillow Home Value Index (ZHVF), Zillow Home Value Forecast (ZHVF), Days To Pending, Inventory levels, etc.\n",
    "\n",
    "All of the data is quantitative. As I mentioned earlier, each dataset is a feature in its own. All the datasets are indexed by regionID where regionID can correspond to one of either a metro city, a neighborhood, a ZIP code, a state, etc. For most of the cases, we are using datasets over metro cities. All the datasets are divided over time ranging from 01/31/2000 to 09/30/2023. Most of the data is divided on a monthly basis. Given a name and a date, you can figure out the value (according to the dataset, eg: ZHVI, ZHVF, etc.). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (EDA) - ALL\n",
    "The datasets we used for this project mainly consists of numerical values which are prices of different homes over a period of time and variables which are different regions and states. Since our main focus is to compare prices across different variables over a different period of time, we had to make sure our datasets included exactly the type of data we needed and we needed to clean to filter out any missing values.\n",
    "\n",
    "\n",
    "The 6 main different datasets are The Zillow Home Value Index (ZHVI), the Zillow Home Value Forecast (ZHVF), the Zillow Observed Rent Index (ZORI), For-Sale Inventory, List and Sale Prices, Sales Count, and Price Cuts. Each dataset is also divided into multiple data types across different geography values such as Metro & U.S, State, County, City, Zip Code and Neighbourhood. However, the issue we faced was that not all datasets have the each of the geography value except for Metro & U.S so we decided to mainly use the datasets which have the Metro & U.S values.\n",
    "\n",
    "We would be plotting 5 graphs of 5 datasets (out of many) that we used. All 5 datasets have the same arrangement. They have the following columns:\n",
    "* RegionID: ID of the region (region can be metro/neighborhood/ZIP region/etc) they have data about\n",
    "* SizeRank: Rank of the area they have data about. All the datasets are sorted by the rank in ascending order.\n",
    "* RegionName: Name of the region (region can metro/neighborhood/ZIP region/etc).\n",
    "* RegionType: Type of region (region can metro/neighborhood/ZIP region/etc). For example, metro cities have the value \"msa\", while ZIP regions have the value \"ZIP\"; states have the value \"state\", and so on.\n",
    "* StateName: Shortform of the state name. NOTE: If RegionType = state, then this column would be empty.\n",
    "* DATES**: This is not a single column but multiple columns. These columns are dates in the format yyyy-mm-dd. The values for these columns is the actual value the dataset is comprised off.\n",
    "\n",
    "For all the graphs, we are plotting the first entry which is the cumulative of whichever attribute the dataset corresponds to over entire U.S. For example, if it is the ZHVI dataset, then the first value (which always is United States) would be plotted against time to see the trend in the entire US. For a specific region, we would have to find the entry with that specific region (can use pandasql) and then plot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a helper function used across all the EDAs\n",
    "def get_x_y(dataframe):\n",
    "    first_row = dataframe.iloc[0]\n",
    "    x_row = first_row.index[5:].to_numpy()\n",
    "    x_row = first_row.iloc[5:].to_numpy()\n",
    "\n",
    "    return [x_row, x_row]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA 1: Zillow Home Value Index (ZHVI)\n",
    "### (Arpan)\n",
    "Zillow Home Value Index (ZHVI): A measure of the typical home value and market changes across a given region and housing type. It reflects the typical value for homes in the 35th to 65th percentile range. Available as a smoothed, seasonally adjusted measure and as a raw measure. We used the smoothed version of data. This is a quantitative data spread across time ranging from 01/31/2000 to 09/30/2023. For any given month, you can check up the value index of a given metro city.\n",
    "\n",
    "Here is the code to generate a plot of the overall ZHVI of properties across the U.S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "# The dataset\n",
    "zhvi_trends = pd.read_csv(\"datasets\\Metro_zhvi_uc_sfrcondo_tier_0.33_0.67_sm_sa_month.csv\")\n",
    "\n",
    "# Getting the X and Y values that will be plotted\n",
    "[x_zhvi, y_zhvi] = get_x_y(zhvi_trends)\n",
    "\n",
    "# Extract years from X\n",
    "years = [datetime.strptime(date, '%Y-%m-%d').year for date in x_zhvi]\n",
    "\n",
    "# Plot the line chart\n",
    "plt.plot(years, y_zhvi, marker='o', linestyle='-', color='b')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('ZHVI')\n",
    "plt.title('Year vs ZHVI')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see in the graph, the trend of ZHVI is progressively growing indicating that the value index of properties across U.S have continued to rise. This can also imply there was a significant increase in the price of the properties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA 2: For-Sale Inventory\n",
    "### (Devesh)\n",
    "The count of unique listings that were active at any time in a given month. This data is again a quantitative data spread across 03/31/2018 to 09/30/2023. For a given metro city, at any given month, this data can give how many properties were listed for sale during that month in that region.\n",
    "\n",
    "Here is plot of inventories across the U.S for all available times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "# The dataset\n",
    "fs_inventory_trends = pd.read_csv(\"datasets\\Metro_invt_fs_uc_sfrcondo_sm_month.csv\")\n",
    "\n",
    "# Getting the X and Y values that will be plotted\n",
    "[x_fs, y_fs] = get_x_y(fs_inventory_trends)\n",
    "\n",
    "# Extract years from X\n",
    "years = [datetime.strptime(date, '%Y-%m-%d').year for date in x_fs]\n",
    "\n",
    "# Plot the line chart\n",
    "plt.plot(years, y_fs, marker='o', linestyle='-', color='b')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('For Sale Inventory (10^6)')\n",
    "plt.title('Year vs Inventory')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can observe, the inventory levels went down progressively. This tells us the there was a decrease in the sales of properties. Practically, we still see people living and population of cities increasing. We can easily infer that there has been a shift from sales of properties to rental properties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA 3: Zillow Observed Rent Index (ZORI)\n",
    "### (Htin Linn)\n",
    "Zillow Observed Rent Index (ZORI): A smoothed measure of the typical observed market rate rent across a given region. ZORI is a repeat-rent index that is weighted to the rental housing stock to ensure representativeness across the entire market, not just those homes currently listed for-rent. The index is dollar-denominated by computing the mean of listed rents that fall into the 40th to 60th percentile range for all homes and apartments in a given region, which is once again weighted to reflect the rental housing stock. This quantitative data also follows the same format as the previous datasets. The range for this dataset is from 01/31/2015 to 09/30/2023.\n",
    "\n",
    "Here is a plot of ZORI across U.S for all available times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "# The dataset\n",
    "zori_trends = pd.read_csv(\"datasets\\Metro_zori_sm_month.csv\")\n",
    "\n",
    "# Getting the X and Y values that will be plotted\n",
    "[x_zori, y_zori] = get_x_y(zori_trends)\n",
    "\n",
    "# Extract years from X\n",
    "years = [datetime.strptime(date, '%Y-%m-%d').year for date in x_zori]\n",
    "\n",
    "# Plot the line chart\n",
    "plt.plot(years, y_zori, marker='o', linestyle='-', color='b')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('ZORI')\n",
    "plt.title('Year vs ZORI')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the ZORI has rised overall and that too drastically. One other interesting thing is that it resembles a staircase pattern of rising which means, the changes were sudden. What caused the spikes can be its own study. But one thing we can infer for sure is that there was definitely rise in rental properties which in turn resulted in a higher value index for rental properties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA 4: Days to Close (Mean)\n",
    "### (Jay)\n",
    "Days to Close: Number of days between the listing going pending and the sale date. This quantitative data is ranged across 01/31/2018 to 08/31/2023.\n",
    "\n",
    "Here is the plot of Days to Close across U.S for the given range of time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "# The dataset\n",
    "days_to_close = pd.read_csv(\"datasets\\Metro_mean_days_to_close_uc_sfrcondo_month.csv\")\n",
    "\n",
    "# Getting the X and Y values that will be plotted\n",
    "[x_dtc, y_dtc] = get_x_y(days_to_close)\n",
    "\n",
    "# Extract years from X\n",
    "years = [datetime.strptime(date, '%Y-%m-%d').year for date in x_dtc]\n",
    "\n",
    "# Plot the line chart\n",
    "plt.plot(years, y_dtc, marker='o', linestyle='-', color='b')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Days to Close')\n",
    "plt.title('Year vs Days-to-Close')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the graph, we can see that there is no specific pattern. But by looking at Y axis, we can infer that usually it takes approx. 30-40 days for a property to close. We see more values around 37 over the years which might make it the avg number of days it takes for a property to close."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA 5: Sales-to-List Ratio (Mean/Median)\n",
    "### (Kevin)\n",
    "Sale-to-List Ratio (mean/median): Ratio of sale vs. final list price. This also is a quantitative data ranging from 01/31/2018 to 08/31/2023 for mean and median both. This might tell us whether there was a change in the negiotiation frame or how frequent it might have become.\n",
    "\n",
    "Here are two plots (mean and median) to observe those trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "# The dataset\n",
    "sale_to_list_ratio_mean = pd.read_csv(\"datasets\\Metro_mean_sale_to_list_uc_sfrcondo_month.csv\")\n",
    "\n",
    "# Getting the X and Y values that will be plotted\n",
    "[x_stlr_mean, y_stlr_mean] = get_x_y(sale_to_list_ratio_mean)\n",
    "\n",
    "# Extract years from X\n",
    "years = [datetime.strptime(date, '%Y-%m-%d').year for date in x_stlr_mean]\n",
    "\n",
    "# Plot the line chart\n",
    "plt.plot(years, y_stlr_mean, marker='o', linestyle='-', color='b')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Sales to List Ratio (Mean)')\n",
    "plt.title('Year vs Sales-to-List-Ratio (Mean)')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "# The dataset\n",
    "sale_to_list_ratio_median = pd.read_csv(\"datasets\\Metro_median_sale_to_list_uc_sfrcondo_sm_month.csv\")\n",
    "\n",
    "# Getting the X and Y values that will be plotted\n",
    "[x_stlr_median, y_stlr_median] = get_x_y(sale_to_list_ratio_median)\n",
    "\n",
    "# Extract years from X\n",
    "years = [datetime.strptime(date, '%Y-%m-%d').year for date in x_stlr_median]\n",
    "\n",
    "# Plot the line chart\n",
    "plt.plot(years, y_stlr_median, marker='o', linestyle='-', color='b')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Sales to List Ratio (Median)')\n",
    "plt.title('Year vs Sales-to-List-Ratio (Median)')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both of the graphs almost look similar, except for the time period 2020-2022. Mean suggests a higher Sales-to-List-Ratio for the year 2021. It might imply there were outliers. Less of the differences, more on the trends; we can see that there's not as such a visible pattern. There was a time period when sales-to-list ratio was high, but as of now it has again dropped."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization (Needed at least 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization 1 (Arpan)\n",
    "\n",
    "### Hypothesis\n",
    "There is a co-relation between schools and home values in a given area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in case you don't have this library installed\n",
    "# %pip install openpyxl\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load ZHVI data\n",
    "zhvi_data = pd.read_csv(\"datasets\\Zip_zhvi_uc_sfrcondo_tier_0.33_0.67_sm_sa_month.csv\")\n",
    "zhvi_data = zhvi_data.drop('SizeRank', axis=1)\n",
    "zhvi_data = zhvi_data.drop('StateName', axis=1)\n",
    "zhvi_data = zhvi_data.rename(columns={'RegionName': 'ZIP'})  # Rename the 'RegionName' column to 'ZIP' in zhvi_data\n",
    "\n",
    "# Load school data\n",
    "school_data = pd.read_excel(\"datasets\\EDGE_GEOCODE_PUBLICSCH_2122.xlsx\")\n",
    "\n",
    "# Creating two groups so that we can compare them on the graph and prove/disprove our hypothesis\n",
    "zhvi_with_schools = zhvi_data.merge(\n",
    "    school_data[['ZIP', 'STATE', 'NAME']],\n",
    "    on='ZIP',\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "# Create 'zhvi_without_schools' dataframe\n",
    "zhvi_without_schools = zhvi_data[~zhvi_data['ZIP'].isin(school_data['ZIP'])]\n",
    "\n",
    "# Select the columns of interest for both dataframes\n",
    "columns_of_interest = [\n",
    "    'RegionID', 'ZIP', 'State', 'City', 'Metro', 'CountyName', '2023-09-30'\n",
    "]\n",
    "\n",
    "zhvi_with_schools = zhvi_with_schools[columns_of_interest + ['NAME']]  # adding name of the schools for more clarity\n",
    "zhvi_without_schools = zhvi_without_schools[columns_of_interest]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(zhvi_with_schools['2023-09-30'], bins=20, alpha=0.5, color='blue', label='With Schools')\n",
    "plt.hist(zhvi_without_schools['2023-09-30'], bins=20, alpha=0.5, color='red', label='Without Schools')\n",
    "plt.xlabel('Median Home Value (ZHVI) at 10^6')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of ZHVI with and without Schools')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "If we look at the Bar chart plot, it is very obvious that there is a huge difference between the two groups: Houses with schools in the same neighborhood (ZIP Code), and Houses without schools in the same neighborhood (ZIP Code). It is observed that houses in the same ZIP Code region as a school had dramatically high Zillow Home Value Index (ZHVI) as of 09/30/2023 than those of houses in a ZIP Code region with no schools. This proves my hypothesis that there is a co-relation between houses and schools. The conclusion is that houses with schools nearby have a higher ZHVI. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization 2 (Devesh)\n",
    "### Hypothesis\n",
    "There is a correlation between ZHVI and ZORI, suggesting that home values influence rental prices?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "zhvi_data = pd.read_csv(\"datasets\\Metro_zhvi_uc_sfrcondo_tier_0.33_0.67_sm_sa_month.csv\")\n",
    "zori_data = pd.read_csv(\"datasets\\Metro_zori_sm_month.csv\")\n",
    "\n",
    "# For simplicity, we'll focus on the most recent month's data for both indices, which is September 2023.\n",
    "# Extract the most recent ZHVI and ZORI values for each region\n",
    "zhvi_latest = zhvi_data[['RegionName', '2023-09-30']].rename(columns={'2023-09-30': 'ZHVI'})\n",
    "zori_latest = zori_data[['RegionName', '2023-09-30']].rename(columns={'2023-09-30': 'ZORI'})\n",
    "\n",
    "# Merge the datasets on the RegionName field\n",
    "combined_zhvi_zori = pd.merge(zhvi_latest, zori_latest, on='RegionName')\n",
    "\n",
    "# Calculate the correlation between ZHVI and ZORI for the merged data\n",
    "correlation = combined_zhvi_zori[['ZHVI', 'ZORI']].corr()\n",
    "\n",
    "# We will plot only the top 10 most populous regions for clarity in the visualization\n",
    "top_regions = combined_zhvi_zori.nlargest(10, 'ZHVI')\n",
    "\n",
    "# Create scatter plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.regplot(x='ZHVI', y='ZORI', data=top_regions)\n",
    "\n",
    "plt.title('Scatter Plot of ZHVI vs ZORI for Top 10 Regions')\n",
    "plt.xlabel('Zillow Home Value Index (ZHVI)')\n",
    "plt.ylabel('Zillow Observed Rent Index (ZORI)')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "The scatter plot with a trend line for the top 10 regions by home value index (ZHVI) has been created, showing a strong positive correlation between the Zillow Home Value Index (ZHVI) and the Zillow Observed Rent Index (ZORI) with a correlation coefficient of approximately 0.88. This suggests a significant relationship where, generally, as home values increase, rental prices also tend to increase in these regions.​​"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization 3 (Htin Linn)\n",
    "### Hypothesis\n",
    "Listings that undergo a price cut stay on the market longer and have a lower Sale-to-List Ratio, suggesting that initial pricing is crucial for a quick sale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Reading Data\n",
    "daystoclose = pd.read_csv('datasets\\Metro_mean_days_to_close_uc_sfrcondo_month.csv')\n",
    "saletolist = pd.read_csv('datasets\\Metro_mean_sale_to_list_uc_sfrcondo_month.csv')\n",
    "\n",
    "# Most Recent Data\n",
    "dtc_latest = daystoclose[['RegionName', '2023-08-31']].rename(columns={'2023-08-31': 'dtc'})\n",
    "stl_latest = saletolist[['RegionName', '2023-08-31']].rename(columns={'2023-08-31': 'stl'})\n",
    "\n",
    "# Box plots\n",
    "plt.figure(figsize=(10, 6))  \n",
    "sns.boxplot(data=dtc_latest, palette=\"Set3\")\n",
    "plt.title(\"Box Plot of Days to Close for Homes with and without Price Cuts\")\n",
    "plt.xlabel(\"Price Cut\")\n",
    "plt.ylabel(\"Mean Days to Close\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))  \n",
    "sns.boxplot(data=stl_latest, palette=\"Set1\")\n",
    "plt.title(\"Box Plot of Sale-to-List Ratio for Homes with and without Price Cuts\")\n",
    "plt.xlabel(\"Price Cut\")\n",
    "plt.ylabel(\"Sale-to-List Ratio\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "The first box plot compares 'Days to Close' for homes with and without price cuts, while the second one compares the 'Sale-to-List Ratio' for the same groups. As we can see from the box plots, most of the listings that undergo a price cut stay on the market longer and have a lower Sale-to-List Ratio, suggesting that initial pricing is crucial for a quick sale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization 4 (Jay)\n",
    "### Hypothesis\n",
    "Do ZHVI and for-sale inventory levels exhibit seasonal patterns, and if so, how do they vary by region and housing tier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import seaborn as sns\n",
    "\n",
    "# Loading the CSV file into a Pandas DataFrame\n",
    "inventory = pd.read_csv(\"datasets\\Metro_invt_fs_uc_sfrcondo_sm_month.csv\")\n",
    "days_pending = pd.read_csv(\"datasets\\Metro_mean_days_to_close_uc_sfrcondo_month.csv\")\n",
    "\n",
    "# Melt the datasets to convert them from wide to long format\n",
    "inventory_melted = inventory.melt(id_vars=['RegionID', 'RegionName'], var_name='Date', value_name='Inventory')\n",
    "days_on_market_melted = days_pending.melt(id_vars=['RegionID', 'RegionName'], var_name='Date', value_name='DaysOnMarket')\n",
    "first_five_regions = inventory['RegionName'][2:6].tolist()\n",
    "\n",
    "inventory_first_five = inventory_melted[inventory_melted['RegionName'].isin(first_five_regions)]\n",
    "days_on_market_first_five = days_on_market_melted[days_on_market_melted['RegionName'].isin(first_five_regions)]\n",
    "merged_data_first_five = pd.merge(inventory_first_five, days_on_market_first_five, on=['RegionName', 'Date'])\n",
    "\n",
    "# Convert 'Date' to datetime and sort the data\n",
    "merged_data_first_five['Date'] = pd.to_datetime(merged_data_first_five['Date'], errors='coerce')\n",
    "\n",
    "merged_data_first_five = merged_data_first_five.dropna(subset=['Date']).sort_values(['RegionName', 'Date'])\n",
    "merged_data_first_five.dropna(subset=['Inventory', 'DaysOnMarket'], inplace=True)\n",
    "\n",
    "# Set style\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "X = merged_data_first_five['Inventory'].values.reshape(-1, 1)\n",
    "Y = merged_data_first_five['DaysOnMarket'].values\n",
    "\n",
    "# linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X, Y)\n",
    "\n",
    "# values for the trend line\n",
    "trend_line = model.predict(X)\n",
    "\n",
    "# scatter plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X, Y, color='c', label='Data points')\n",
    "plt.plot(X, trend_line, color='blue', label='Trend Line')\n",
    "plt.title('Correlation between Inventory Levels and Days on Market for the US (Top 5 regions)')\n",
    "plt.xlabel('Inventory Levels')\n",
    "plt.ylabel('Days on Market')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "There are two CSV files used for this visualization, Inventory.csv and days_pending.csv. The data from both CSV files is transformed from a wide format to a long format using the melt method. This restructuring makes it easier to manipulate and analyze the data. The code filters out the data for the top five regions in the inventory dataset. It then merges these filtered datasets on the basis of region names and dates, creating a combined dataset that does a comparative analysis between inventory levels and days on the market for these specific regions. The 'Date' column is converted to a datetime format. Any rows with missing values in the 'Date', 'Inventory', or 'DaysOnMarket' columns are dropped. A seaborn style setting is applied just for getting the grid for the plot. The inventory and days on market data are prepared for plotting and regression analysis. A linear regression model is fitted to the data to understand the relationship between inventory levels and the days on market. The code then predicts values for a trend line based on this model, which will be plotted alongside the actual data points. A scatter plot is created to visualize the relationship between the inventory levels and the days on the market for these regions. The trend line from the linear regression analysis is also plotted to illustrate the general direction of the relationship.\n",
    "\n",
    "We found out that the correlation is positive which suggests that it has been a hot market in the top 5 regions of the United States from 2018 to present. Both low inventory and quick sales could occur simultaneously, indicating high demand and a fast-moving market."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization 5 (Kevin)\n",
    "### Hypothesis\n",
    "The 4 bedroom home values are growing at a faster rate than the home values with lesser bedrooms, indicating an increasing disparity in the housing market."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load your data into pandas DataFrames from CSV files\n",
    "one_bed = pd.read_csv('datasets\\Metro_zhvi_bdrmcnt_1_uc_sfrcondo_tier_0.33_0.67_sm_sa_month.csv')\n",
    "two_bed = pd.read_csv('datasets\\Metro_zhvi_bdrmcnt_2_uc_sfrcondo_tier_0.33_0.67_sm_sa_month.csv')\n",
    "three_bed = pd.read_csv('datasets\\Metro_zhvi_bdrmcnt_3_uc_sfrcondo_tier_0.33_0.67_sm_sa_month.csv')\n",
    "four_bed = pd.read_csv('datasets\\Metro_zhvi_bdrmcnt_4_uc_sfrcondo_tier_0.33_0.67_sm_sa_month.csv')\n",
    "\n",
    "chicago_one = one_bed[one_bed['RegionName'] == 'Chicago, IL']\n",
    "chicago_one_from_2000_to_end = chicago_one.iloc[0][one_bed.columns.get_loc('2000-01-31'):]\n",
    "chicago_two = two_bed[two_bed['RegionName'] == 'Chicago, IL']\n",
    "chicago_two_from_2000_to_end = chicago_two.iloc[0][two_bed.columns.get_loc('2000-01-31'):]\n",
    "chicago_three = three_bed[three_bed['RegionName'] == 'Chicago, IL']\n",
    "chicago_three_from_2000_to_end = chicago_three.iloc[0][three_bed.columns.get_loc('2000-01-31'):]\n",
    "chicago_four = four_bed[four_bed['RegionName'] == 'Chicago, IL']\n",
    "chicago_four_from_2000_to_end = chicago_four.iloc[0][four_bed.columns.get_loc('2000-01-31'):]\n",
    "\n",
    "# Increase default font size and line width for better readability\n",
    "sns.set_context('talk', font_scale=0.8)\n",
    "\n",
    "# Set the Seaborn theme\n",
    "sns.set_style('whitegrid')\n",
    "sns.set_palette('pastel')\n",
    "\n",
    "# Create the figure and the line plot\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "# Plot the data for each bedroom type\n",
    "for df, label in [(chicago_one_from_2000_to_end, '1 Bedroom'),\n",
    "                 (chicago_two_from_2000_to_end, '2 Bedroom'),\n",
    "                 (chicago_three_from_2000_to_end, '3 Bedroom'),\n",
    "                 (chicago_four_from_2000_to_end, '4 Bedroom')]:\n",
    "\n",
    "    # Convert index to datetime\n",
    "    dates = pd.to_datetime(df.index)\n",
    "\n",
    "    # Plot with Seaborn\n",
    "    sns.lineplot(x=dates, y=df.values, label=label, linewidth=2.5)\n",
    "\n",
    "# Set the x-axis to display years only\n",
    "plt.gca().xaxis.set_major_locator(mdates.YearLocator())\n",
    "plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "\n",
    "# Set y-axis limits and labels\n",
    "plt.ylim(50000, plt.gca().get_ylim()[1]) # Set minimum y-value to 50000 and maximum to auto\n",
    "y_max = int(plt.gca().get_ylim()[1] // 50000 * 50000) # Find the nearest number less than max that is divisible by 50000\n",
    "if y_max < plt.gca().get_ylim()[1]: # If the max y-value is not a multiple of 50000, add another step\n",
    "    y_max += 50000\n",
    "plt.yticks(np.arange(50000, y_max, 50000))\n",
    "\n",
    "# Format the y-axis labels to include commas for thousands\n",
    "plt.gca().get_yaxis().set_major_formatter(plt.matplotlib.ticker.FuncFormatter(lambda x, p: format(int(x), ',')))\n",
    "\n",
    "# Rotate x-axis labels if needed\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Set plot title and labels\n",
    "plt.title('Chicago Home Value Trends by Bedroom Count', fontsize=18)\n",
    "plt.xlabel('Year', fontsize=16)\n",
    "plt.ylabel('ZHVI (USD)', fontsize=16)\n",
    "\n",
    "# Show legend with a title and increase its font size\n",
    "plt.legend(title='Bedroom Count', title_fontsize='14', fontsize='12', loc='upper left')\n",
    "\n",
    "# Set the background color\n",
    "plt.gca().set_facecolor('#f5f5f5')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Analysis (needed at least 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Analysis 1 (Devesh)\n",
    "### ML Use Case\n",
    "Develop a predictive model that forecasts home values (ZHVI) based on various features such as location, home size, number of bedrooms, and historical trends. This is a regression problem where the target variable is continuous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Load the dataset\n",
    "zhvi_data = pd.read_csv(\"datasets\\Metro_zhvi_uc_sfrcondo_tier_0.33_0.67_sm_sa_month.csv\")\n",
    "\n",
    "# Impute missing values using the median\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "model_data_numeric = zhvi_data.select_dtypes(include=[np.number])  # Select only numeric columns\n",
    "model_data_imputed = imputer.fit_transform(model_data_numeric)\n",
    "model_data_imputed_df = pd.DataFrame(model_data_imputed, columns=model_data_numeric.columns)\n",
    "\n",
    "# Prepare the historical ZHVI data for modeling\n",
    "# We'll use the past 12 months of ZHVI values as features to predict the next month's ZHVI value\n",
    "X = model_data_imputed_df.iloc[:, :-13].values\n",
    "y = model_data_imputed_df.iloc[:, -13].values  # We use -13 to predict the ZHVI 12 months ahead\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Initialize the Gradient Boosting Regressor model\n",
    "gbr = GradientBoostingRegressor()\n",
    "\n",
    "# Train the model on the scaled dataset\n",
    "gbr.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict the ZHVI values on the test set using the trained model\n",
    "y_pred_gbr = gbr.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model using the test set\n",
    "mae_gbr = mean_absolute_error(y_test, y_pred_gbr)\n",
    "rmse_gbr = np.sqrt(mean_squared_error(y_test, y_pred_gbr))\n",
    "r2_gbr = r2_score(y_test, y_pred_gbr)\n",
    "\n",
    "print(mae_gbr, rmse_gbr, r2_gbr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Analysis 2 (Arpan)\n",
    "### ML Use Case\n",
    "Classify regional housing markets as \"hot,\" \"stable,\" or \"cold\" based on current market indicators such as Days to Pending, Inventory Levels, Sale-to-List Ratio, and historical ZHVI trends. This is a classification problem where the target variable is categorical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Data Acquisition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "\n",
    "# Would be looking at four features specifically\n",
    "# [Days_to_Pending, Inventory_Levels, Sale_to_List_Ratio, ZHVI_Trends, Market_Temperature (Target/Label)]\n",
    "days_to_pending = pd.read_csv(\"datasets\\Metro_med_doz_pending_uc_sfrcondo_sm_month.csv\")\n",
    "inventory_levels = pd.read_csv(\"datasets\\Metro_invt_fs_uc_sfrcondo_sm_month.csv\")\n",
    "sale_to_list_ratio = pd.read_csv(\"datasets\\Metro_median_sale_to_list_uc_sfrcondo_sm_month.csv\")\n",
    "zhvi_trends = pd.read_csv(\"datasets\\Metro_zhvi_uc_sfrcondo_tier_0.33_0.67_sm_sa_month.csv\")\n",
    "sales_count = pd.read_csv(\"datasets\\Metro_sales_count_now_uc_sfrcondo_month.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Would be cleaning out data and making it balanced.\n",
    "# Firstly considering the same time for data.\n",
    "# ZHVI has more data than other data sets. So we would be not considering a lot of columns from that dataframe.\n",
    "# We want to consider data from '2018-03-31' to '2023-09-30' (Undersampling)\n",
    "# inventory_levels end at '2023'08'31', so need to impute values for that column\n",
    "date_range = pd.date_range(start='2018-03-31', end='2023-09-30', freq='M')\n",
    "\n",
    "# Cleaning days_to_pending\n",
    "days_to_pending = days_to_pending.drop('SizeRank', axis=1)\n",
    "days_to_pending = days_to_pending[days_to_pending['RegionName'] != 'United States']\n",
    "# print(\"days_to_pending\")\n",
    "# print(days_to_pending.columns)  # all columns (allegedly across all dataframes)\n",
    "\n",
    "# Cleaning inventory_levels\n",
    "inventory_levels = inventory_levels.drop('SizeRank', axis=1)\n",
    "inventory_levels = inventory_levels[inventory_levels['RegionName'] != 'United States']\n",
    "# Imputing data in inventory_levels\n",
    "# Calculate the median of the existing values in the DataFrame for the '2023-09-30' column\n",
    "column_index_i_l = inventory_levels.columns.get_loc('2018-03-31') # Find the index of the '2018-03-31' column\n",
    "median_value = inventory_levels.iloc[:, column_index_i_l:-1].median(axis=1)  # column_index_i_l represents the index of the '2018-03-31' column\n",
    "inventory_levels['2023-09-30'] = median_value  # Add the '2023-09-30' column and impute placeholder values (e.g., 0) for now\n",
    "# print(days_to_pending.columns == inventory_levels.columns) # [... True ...]\n",
    "\n",
    "# Cleaning sale_to_list_ratio\n",
    "sale_to_list_ratio = sale_to_list_ratio.drop('SizeRank', axis=1)\n",
    "sale_to_list_ratio = sale_to_list_ratio[sale_to_list_ratio['RegionName'] != 'United States']\n",
    "# Imputing data in sale_to_list_ratio\n",
    "# Calculate the median of the existing values in the DataFrame for the '2023-09-30' column\n",
    "column_index_s_t_l = sale_to_list_ratio.columns.get_loc('2018-03-31') # Find the index of the '2018-03-31' column\n",
    "median_value = sale_to_list_ratio.iloc[:, column_index_s_t_l:-1].median(axis=1)  # column_index_s_t_l represents the index of the '2018-03-31' column\n",
    "sale_to_list_ratio['2023-09-30'] = median_value  # Add the '2023-09-30' column and impute placeholder values (e.g., 0) for now\n",
    "# print(\"\\nsale_to_list_ratio\")\n",
    "# print(inventory_levels.columns == sale_to_list_ratio.columns)  # [... True ...]\n",
    "\n",
    "# Cleaning zhvi_trends\n",
    "zhvi_trends = zhvi_trends.drop('SizeRank', axis=1)\n",
    "zhvi_trends = zhvi_trends[zhvi_trends['RegionName'] != 'United States']\n",
    "zhvi_trends = zhvi_trends[['RegionID', 'RegionName', 'RegionType', 'StateName'] + date_range.strftime('%Y-%m-%d').tolist()]\n",
    "# print(\"\\nzhvi_trends\")\n",
    "# print(sale_to_list_ratio.columns == zhvi_trends.columns)  # [... True ...]\n",
    "\n",
    "# Cleaning sales_count\n",
    "sales_count = sales_count.drop('SizeRank', axis=1)\n",
    "sales_count = sales_count[sales_count['RegionName'] != 'United States']\n",
    "sales_count = sales_count[['RegionID', 'RegionName', 'RegionType', 'StateName'] + date_range.strftime('%Y-%m-%d').tolist()]\n",
    "# print(zhvi_trends.columns == sales_count.columns)  # [... True ...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Data Processing\n",
    "Creating another dataframe market_temperature which would be our Target label market temperature determines whether the market is hot/stable/cold. In real world it is based on a lot of features, but for this project we are only considering two features to determine market temperature: sales count and inventory levels.\n",
    "\n",
    "$ market\\_temperature = sales\\_count / inventory\\_levels $\n",
    "\n",
    "* If low inventory yet more sales, implies there's more demand in the market, hence the market is 'hot'.\n",
    "* If moderate inventory and moderate sales, implies the market is 'stable'.\n",
    "* If high inventory yet less sales, implies there's less demand in the market, hence the market is'cold'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have sales_count and inventory_levels, we can create market_temperature\n",
    "# Define the common columns for merging\n",
    "common_columns = ['RegionID', 'RegionName', 'RegionType', 'StateName']\n",
    "\n",
    "# Merge the dataframes on the common columns\n",
    "market_temperature = pd.merge(sales_count, inventory_levels, on=common_columns, suffixes=('_sales', '_inventory'))\n",
    "\n",
    "# Iterate over the date columns to calculate market temperature\n",
    "for column in sales_count.columns.difference(common_columns):\n",
    "    market_temperature[column] = market_temperature[f\"{column}_sales\"] / market_temperature[f\"{column}_inventory\"]\n",
    "\n",
    "# Drop the redundant columns\n",
    "market_temperature = market_temperature.drop(market_temperature.filter(like='_sales').columns, axis=1)\n",
    "market_temperature = market_temperature.drop(market_temperature.filter(like='_inventory').columns, axis=1)\n",
    "\n",
    "# Now we have the market_temperature in values\n",
    "# market_temperature.head(5)\n",
    "\n",
    "# Define the mapping function\n",
    "def map_to_category(value):\n",
    "    if pd.notna(value):\n",
    "        if value <= 0.333333:\n",
    "            return 'cold'\n",
    "        elif value <= 0.666666:\n",
    "            return 'stable'\n",
    "        else:\n",
    "            return 'hot'\n",
    "    return value\n",
    "\n",
    "# Apply the mapping function to convert decimal values to categories\n",
    "market_temperature_categorical = market_temperature.copy()\n",
    "market_temperature_categorical.iloc[:, 4:] = market_temperature.iloc[:, 4:].apply(lambda x: x.map(map_to_category))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Machine Learning (Decision Tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Features: [Days_to_Pending, Inventory_Levels, Sale_to_List_Ratio, ZHVI_Trends]\n",
    "X = market_temperature[['2018-03-31', '2018-04-30', '2018-05-31', '2018-06-30', '2018-07-31', '2018-08-31',\n",
    "                        '2018-09-30', '2018-10-31', '2018-11-30', '2018-12-31', '2019-01-31', '2019-02-28',\n",
    "                        '2019-03-31', '2019-04-30', '2019-05-31', '2019-06-30', '2019-07-31', '2019-08-31',\n",
    "                        '2019-09-30', '2019-10-31', '2019-11-30', '2019-12-31', '2020-01-31', '2020-02-29',\n",
    "                        '2020-03-31', '2020-04-30', '2020-05-31', '2020-06-30', '2020-07-31', '2020-08-31',\n",
    "                        '2020-09-30', '2020-10-31', '2020-11-30', '2020-12-31', '2021-01-31', '2021-02-28',\n",
    "                        '2021-03-31', '2021-04-30', '2021-05-31', '2021-06-30', '2021-07-31', '2021-08-31',\n",
    "                        '2021-09-30', '2021-10-31', '2021-11-30', '2021-12-31', '2022-01-31', '2022-02-28',\n",
    "                        '2022-03-31', '2022-04-30', '2022-05-31', '2022-06-30', '2022-07-31', '2022-08-31',\n",
    "                        '2022-09-30', '2022-10-31', '2022-11-30', '2022-12-31', '2023-01-31', '2023-02-28',\n",
    "                        '2023-03-31', '2023-04-30', '2023-05-31', '2023-06-30', '2023-07-31', '2023-08-31',\n",
    "                        '2023-09-30']]\n",
    "\n",
    "# Target variable: Market_Temperature (categorical)\n",
    "y = market_temperature_categorical['2023-09-30']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# Initialize the Decision Tree model\n",
    "dt_model = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Train the model\n",
    "dt_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = dt_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "classification_rep = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(\"Classification Report:\\n\", classification_rep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reflection\n",
    "\n",
    "### 1. What is the most challenging part of the project that you’ve encountered so far?\n",
    "\n",
    "In terms of engineering the machine learning part was most difficult, mostly because the data is multi-modal and multi-dimensional. Furthermore, there was a significant imbalance in data. ZHVI had more time-span of data than others, which made us undersample to focus on the common time. Furthermore, we also had to do data imputation as some datasets did not have a 09/30/2023 column, especially for machine learning to have balanced data. Lastly, once we processed the data, training the model on this complex architecture of datasets was hard.\n",
    "\n",
    "### 2. What are your initial insights?\n",
    "\n",
    "Initally, we had a very different project, and to us it seemed very practical. While we are saddened by not finding enough data for that, this idea was really insightful as well. We had lots of data, which enabled us to independently research our own interests within the realm of real-estate market.\n",
    "\n",
    "### 3. Are there any concrete results you can show at this point? If not, why not?\n",
    "\n",
    "Almost everything in this progress report is concrete, except for Arpan's machine learning. While it works, we believe it can be further improved to consider all the features to classify market temperature. Once it works accurately as a classifier, we can even make it interactive by providing bunch of features and the model outputs whether the market is hot/stable/cold. Intuitively, this can also be used as a prediction if we enter predicted features, and the model's classification in which case would also be a prediction technically.\n",
    "\n",
    "### 4. Going forward, what are the current biggest problems you’re facing?\n",
    "\n",
    "Figuring out a way to create a single point of data with all this different datasets. The architecture of this datasets is very complex which makes merging these datasets into one harder. We had initial ideas of how to merge, but it proved to complex to finish by this project report. We would try and see if we can get that done by final submission. If we get that done, we would have even more advanced machine learning model which would be a very effective and powerful classifier.\n",
    "\n",
    "### 5. Do you think you are on track with your project? If not, what parts do you need to dedicate more time to?\n",
    "\n",
    "I think we are on track. We accomplished most of the tasks we setup for ourselves, at least achieved it. Moving forward we would work on perfecting it.\n",
    "\n",
    "### 6. Given your initial exploration of the data, is it worth proceeding with your project, why? If not, how will you move forward (method, data etc)?\n",
    "\n",
    "Yes, this current (new) project idea is definitely worth proceeding. The visualizations are something that property managers actually consider an important insight for their trade. Furthermore, our main goal is to get the machine learning model working. Given the data, the practicality of this idea and our solutions and insights, this project is worth a lot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Step\n",
    "1. Making more visually pleasing visualizations.\n",
    "2. Try to figure out a better way to visualize. Maybe instead of yearly, try to plot the values over a cycle of six months, or even one month.\n",
    "3. Figure out a way to merge all this data into a single dataset.\n",
    "4. Once 3. is done, figure out how to train a model with that much data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
